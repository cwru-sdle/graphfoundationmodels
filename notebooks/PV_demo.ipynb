{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photovoltaic Fleet ddDT Demo\n",
    "This notebook is an example of training a ddDT on a fleet of PV systems.\n",
    "\n",
    "Before running this notebook, make sure the package is installed in your system by running \n",
    "`pip install -e .` from the base directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch_geometric.nn import ChebConv\n",
    "from tqdm import tqdm\n",
    "from rwb_stgnn_functions import *\n",
    "from rwb_dataloader_sunsmart import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorys\n",
    "meta_directory = '/mnt/vstor/CSE_MSE_RXF131/staging/sdle/foundational_models/sunsmart/sunsmart_meta_grade.parquet'\n",
    "\n",
    "# Dataloader\n",
    "pvts = stDataloader(parent_directory = '/mnt/vstor/CSE_MSE_RXF131/staging/sdle/foundational_models/sunsmart/',\n",
    "                        raw_parquet_path = '',\n",
    "                        node_parquet_path ='',\n",
    "                        channel_parquet_path = '',\n",
    "                        adjacency_parquet_path = '', \n",
    "                        meta_parquet_path = meta_directory,\n",
    "                        num_nodes = num_nodes, \n",
    "                        traintestsplit= traintestsplit,\n",
    "                        splittype = splittype,\n",
    "                        normalize = True,\n",
    "                        model_output_directory='',\n",
    "                        test_name = test_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the complexity of this system, an integrated dataloader has been developed that operates off of config files. We'll load that in first.\n",
    "\n",
    "We also define a set folder structure for the data, with optional functionality for data preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel names\n",
    "pvts.channel_names\n",
    "# All dfs currently in channel_directory\n",
    "pvts.channel_dataframes\n",
    "\n",
    "# %% Data Preprocessing\n",
    "# Reads in Raw Data\n",
    "# Optional: splits tidy formatted data into separate csvs\n",
    "# Add df manulipulation feature later\n",
    "# Optional: writes to node_parquet path\n",
    "#split_dfs = pvts.read_from_tabular(df_split_col='invt', write = False)\n",
    "\n",
    "# %% Get Timespan\n",
    "# Determines longest overlapping window\n",
    "# From list of dfs\n",
    "# Defaults self.node_dataframes\n",
    "pvts.read_files_with_nodeID()\n",
    "#timespan = pvts.get_longest_timespan(freq = '15min', tz = 'UTC',set_params = False)\n",
    "\n",
    "#Set Timespan\n",
    "pvts.set_date_range('2012-09-28 07:00:00', '2015-10-08 05:30:00', '15min', tz = 'UTC')\n",
    "\n",
    "# Extracts columns from node df with multiple measurements, resizes based on self.time_index\n",
    "# From list of node dfs\n",
    "# Defaults self.node_dataframes\n",
    "# Optional: write to self.channel_parquet_path\n",
    "real_world_measurements = ['PVDCPOWER']\n",
    "\n",
    "emp_model = ['p_mp']\n",
    "\n",
    "sun_measurements = ['sun_angle']\n",
    "\n",
    "weather_measurements = ['temp_air','dhi',\n",
    "       'dni', 'ghi','wind_speed']\n",
    "\n",
    "#Converts tabular data to matrix\n",
    "_ = pvts.tabular_to_matrix(measurement_cols = real_world_measurements, write = False)\n",
    "_ = pvts.tabular_to_matrix(measurement_cols = emp_model, write = False)\n",
    "_ = pvts.tabular_to_matrix(measurement_cols = sun_measurements, write = False)\n",
    "_ = pvts.tabular_to_matrix(measurement_cols = weather_measurements, write = False)\n",
    "\n",
    "#Calculates adjacency matrix\n",
    "_ = pvts.generate_distance_adjacency_matrix(latitude = 'lat', longitude='lon', epsilons = [0.0,0.2,0.4,0.6,0.8,1.0], write = False, weighted=False)\n",
    "_ = pvts.generate_distance_adjacency_matrix(latitude = 'lat', longitude='lon', epsilons = [0.1,0.3,0.5,0.7,0.9], write = False)\n",
    "\n",
    "_ = pvts.generate_datasetgrade_adjacency_matrix(grade_col = 'sdt_grade', min_grade_threshold = .5, max_diff_thresholds = [0.1,0.2,0.3,0.5,0.75,1.0], write = False)\n",
    "\n",
    "\n",
    "# Extracts columns from node df with multiple measurements, resizes based on self.time_index, checks for NANs\n",
    "# From list of node dfs\n",
    "# Defaults self.node_dataframes\n",
    "# Optional: Write to self.channel_parquet_path\n",
    "_ = pvts.data_availibility(measurement_cols = real_world_measurements, write = False)\n",
    "\n",
    "# %% Regularize Time interval for incommesurate indices \n",
    "# Reindexes timeseries based on dataloader timespan\n",
    "# Only Linear Implemented \n",
    "# Timezone Aware\n",
    "# _ = pvts.ts_reindex_with_interpolation('sun_angle',\n",
    "#                                 start_freq = '15min',\n",
    "#                                 method = 'linear'\n",
    "#                                 )\n",
    "\n",
    "# %% Conditional Masking / Data manipulation\n",
    "# \n",
    "#\n",
    "#\n",
    "# N number of Conditional Statements to apply Masks:\n",
    "#   ([(statement_1),(statement_2),...,(statement_n)], \n",
    "#           channel_if_true, channel_if_false, \n",
    "#           scalar_if_true, scalar_if_false,\n",
    "#           new_channel_name\n",
    "#       )\n",
    "\n",
    "#   if (channel_x, conditional statement, channel_y, scalar value) \n",
    "#   AND \n",
    "#   (statement_2) ... AND (statement_n)\n",
    "#   THEN\n",
    "#   \n",
    "#   [or to compare scalar value]\n",
    "#   if (channel_z, conditional statement 2, None, scalar value)\n",
    "\n",
    "good_data_filter = [([('ghi', np.greater, None, 50), ('PVDCPOWER', np.less, None, 5)], np.all, None, None, 1, 0, \"ghi_fill_flag\")]\n",
    "\n",
    "\n",
    "mask = [([('ghi_fill_flag', np.equal, None, 1), ('PVDCPOWER', pd.isna, None, 1)], np.any, 'p_mp', 'PVDCPOWER', 1, 0, \"cdcp_fill\"),\n",
    "        ([('sun_angle', np.less, None, .20), ('ghi_fill_flag', np.equal, None, 1), ('PVDCPOWER', pd.isna, None, 1)], np.any, None, None, 1, 0, \"mask\")]\n",
    "\n",
    "mask = [([('sun_angle', np.less, None, .05)], np.any, None, None, 1, 0, \"night_mask\")]\n",
    "\n",
    "_ = pvts.apply_conditions(good_data_filter)\n",
    "_ = pvts.apply_conditions(mask, write = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the model inputs. This controls the architecture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inputs\n",
    "inputs = ['cdcp_fill']\n",
    "outputs = ['cdcp_fill']\n",
    "masks = ['mask']\n",
    "adjacency = ['epsilon_0.7']\n",
    "\n",
    "# Model Validation Options\n",
    "traintestsplit = True\n",
    "splittype = 'space'\n",
    "splits = (.75,.20,.05)\n",
    "\n",
    "# Model parameters\n",
    "num_nodes = 90\n",
    "num_channels = 1\n",
    "#Number of Learned Filters per Temporal Conv Block\n",
    "#First number is num \n",
    "channels = [[len(inputs), 9, 18], [18, 9, len(outputs)]]\n",
    "\n",
    "#Temporal Convolutional Kernel Size\n",
    "kernel_size = 4\n",
    "#Temporal Deconvolutional Kernal Size (Second Layer)\n",
    "kernel_size_de = 2\n",
    "\n",
    "#Global Stride, Padding\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "#Chebchev Kernel (n-hops message passing)\n",
    "K = 3\n",
    "\n",
    "#Batch Size\n",
    "batch_size = 1\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 5\n",
    "num_layers = 2\n",
    "\n",
    "#Temporal Window\n",
    "window_size = 2880\n",
    "shuffle = False\n",
    "\n",
    "# Model Setup\n",
    "test_name = 'autoencode_test'\n",
    "model_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define where to save results and model objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Save Paths\n",
    "model_name = 'model_' + str(model_num) + '.pt'\n",
    "model_save_path = pvts.model_path + model_name\n",
    "model_results_name = 'model_' + str(model_num) + '.csv'\n",
    "model_results_path = pvts.model_results_path + model_results_name\n",
    "pred_test_name = 'model_' + str(model_num) + '.parquet'\n",
    "pred_test_save_path = pvts.pred_test_path + pred_test_name\n",
    "pred_name = 'model_' + str(model_num) + '.parquet'\n",
    "pred_save_path = pvts.pred_path + pred_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's configure the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Timespan\n",
    "pvts.set_date_range('2012-09-28 07:00:00', '2015-10-08 05:30:00', '15min', tz = 'UTC')\n",
    "\n",
    "# Set Model Inputs\n",
    "pvts.set_window_size(window_size)\n",
    "pvts.set_model_inputs(inputs, outputs, masks, adjacency)\n",
    "pvts.set_train_test_splits(splits)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally select specific nodes (colums) or time windows to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Specific Train Cols (Optional)\n",
    "pvts.train_cols = [0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 17, 18, 20, 21,\n",
    "       22, 23, 24, 25, 26, 27, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 42,\n",
    "       44, 45, 46, 47, 49, 51, 52, 53, 55, 56, 59, 60, 61, 62, 63, 64, 65,\n",
    "       67, 68, 69, 70, 74, 76, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89]\n",
    "pvts.test_cols = [14, 15, 19, 28, 29, 34, 43, 48, 50, 57, 58, 66, 71, 73, 75, 77, 80,\n",
    "       86]\n",
    "pvts.val_cols = [3, 16, 33, 54, 72]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the tensor datasets, and the model iterable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the Datafrom tables to matrices\n",
    "train_data, test_data, val_data, full_data = pvts.data_to_model_input_transform()\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=int(batch_size),\n",
    "                        shuffle=shuffle, num_workers=1)\n",
    "test_iter = DataLoader(test_data, batch_size=int(batch_size),\n",
    "                        shuffle=False, num_workers=1)\n",
    "val_iter = DataLoader(val_data, batch_size=int(batch_size),\n",
    "                        shuffle=False, num_workers=1)\n",
    "full_iter = DataLoader(full_data, batch_size=int(batch_size),\n",
    "                        shuffle=False, num_workers=1)\n",
    "\n",
    "# %% Transform the adjacency matrix tables to matrices\n",
    "train_w, train_edge_index, train_edge_weight, full_w, full_edge_index, full_edge_weight = pvts.weight_to_model_input_transform(datasplit = 'training')\n",
    "\n",
    "# %% Initializes Model\n",
    "model = STConvAE(device, num_nodes, channels, num_layers, \n",
    "                    kernel_size, K, window_size, kernel_size_de, stride, \n",
    "                    padding, normalization = 'sym', bias = True)\n",
    "                    \n",
    "model = model.to(device)\n",
    "\n",
    "#Loss Functions\n",
    "loss = nn.MSELoss()\n",
    "min_test_loss = np.inf\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "\n",
    "#Model Loss\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, num_epochs + 1), desc = 'Epoch', position = 0):\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    model.train()\n",
    "    # i = 0\n",
    "    \n",
    "    for x, y, mask in tqdm(train_iter, desc = 'Batch', position = 0):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y = y.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y_pred = model(x.to(device), \n",
    "                        train_edge_index.to(device), \n",
    "                        train_edge_weight.to(device)\n",
    "                        ) \n",
    "\n",
    "        loss= torch.nanmean((torch.where(mask == False, \n",
    "                                (y_pred-y)**2, \n",
    "                                torch.tensor(float('nan'))\n",
    "                                ))\n",
    "                                )\n",
    "\n",
    "        if not torch.isnan(loss):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() \n",
    "            train_loss.append(epoch_train_loss)\n",
    "\n",
    "    epoch_test_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    if splittype == 'space':\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y, mask in tqdm(full_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                y_pred = model(x.to(device),\n",
    "                                    full_edge_index.to(device),\n",
    "                                    full_edge_weight.to(device)\n",
    "                                    ) \n",
    "\n",
    "                y = y[:,pvts.test_cols]\n",
    "                mask = mask[:,pvts.test_cols]\n",
    "                y_pred = y_pred[:,pvts.test_cols]\n",
    "                \n",
    "                loss = torch.nanmean((torch.where(mask == False,\n",
    "                                        (y_pred-y)**2,\n",
    "                                        torch.tensor(float('nan'))\n",
    "                                        ))\n",
    "                                        )\n",
    "\n",
    "                if not torch.isnan(loss):\n",
    "                    epoch_test_loss += loss.item()\n",
    "                    test_loss.append(epoch_test_loss)\n",
    "\n",
    "    else: \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for x, y, mask in tqdm(test_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                y_pred = model(x.to(device),\n",
    "                                    full_edge_index.to(device),\n",
    "                                    full_edge_weight.to(device)\n",
    "                                    ) \n",
    "\n",
    "                loss = torch.nanmean((torch.where(mask == False,\n",
    "                                        (y_pred-y)**2,\n",
    "                                        torch.tensor(float('nan'))\n",
    "                                        ))\n",
    "                                        )\n",
    "\n",
    "                if not torch.isnan(loss):\n",
    "                    epoch_test_loss += loss.item()\n",
    "                    test_loss.append(epoch_test_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\n Training Loss: {epoch_train_loss / len(train_iter)} \\n Evaulation Loss: {epoch_test_loss / len(test_iter)}')\n",
    "    \n",
    "    if min_test_loss > epoch_test_loss:\n",
    "        min_test_loss = epoch_test_loss\n",
    "        torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# Intialize Model\n",
    "eval_model = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, window_size, kernel_size_de, stride, padding, normalization = 'sym', bias = True).to(device)\n",
    "eval_model.load_state_dict(torch.load(model_save_path))\n",
    "eval_model = eval_model.to(device)\n",
    "eval_loss = nn.MSELoss()\n",
    "eval_model.eval()\n",
    "\n",
    "if splittype == 'space':\n",
    "\n",
    "    first_full = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for x, y, mask in tqdm(full_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            # get model predictions and compute loss\n",
    "            y_pred = eval_model(x.to(device), \n",
    "                                    full_edge_index.to(device), \n",
    "                                    full_edge_weight.to(device)\n",
    "                                    )\n",
    "            \n",
    "            if first_full == 1:\n",
    "                y_complete = y[:,:,:,0].unsqueeze(-1)\n",
    "                mask_complete = mask\n",
    "                y_pred_complete = y_pred\n",
    "\n",
    "            else:\n",
    "                y_complete = torch.cat((y_complete, y[:,:,:,0].unsqueeze(-1)))\n",
    "                mask_complete = torch.cat((mask_complete, mask))\n",
    "                y_pred_complete = torch.cat((y_pred_complete, y_pred))\n",
    "\n",
    "            first_full+=1\n",
    "\n",
    "    raw = (y_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "    raw = np.where(raw == 0, float(.1), raw)\n",
    "\n",
    "    mask = (mask_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "    pred = (y_pred_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "    \n",
    "    raw = raw[0:len(pvts.time_index)]\n",
    "    mask = mask[0:len(pvts.time_index)]\n",
    "    pred = pred[0:len(pvts.time_index)]\n",
    "\n",
    "    raw_val = raw[:,pvts.val_cols]\n",
    "    mask_val = mask[:,pvts.val_cols]\n",
    "    pred_val = pred[:,pvts.val_cols]\n",
    "\n",
    "elif splittype == 'time':\n",
    "\n",
    "        first_val = 1\n",
    "\n",
    "        for x, y, mask in tqdm(val_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            # get model predictions and compute loss\n",
    "            y_pred = eval_model(x.to(device), \n",
    "                                    full_edge_index.to(device), \n",
    "                                    full_edge_weight.to(device)\n",
    "                                    )\n",
    "            \n",
    "            if first_val == 1:\n",
    "                y_complete = y[:,:,:,0].unsqueeze(-1)\n",
    "                mask_complete = mask\n",
    "                y_pred_complete = y_pred\n",
    "\n",
    "            else:\n",
    "                y_complete = torch.cat((y_complete, y[:,:,:,0].unsqueeze(-1)))\n",
    "                mask_complete = torch.cat((mask_complete, mask))\n",
    "                y_pred_complete = torch.cat((y_pred_complete, y_pred))\n",
    "\n",
    "            first_val+=1\n",
    "\n",
    "        raw_val = (y_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "\n",
    "\n",
    "        mask_val = (mask_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "        pred_val = (y_pred_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "\n",
    "        first_full=1\n",
    "        \n",
    "        for x, y, mask in tqdm(full_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # get model predictions and compute loss\n",
    "            y_pred = eval_model(x.to(device), \n",
    "                                    full_edge_index.to(device), \n",
    "                                    full_edge_weight.to(device)\n",
    "                                    )\n",
    "            \n",
    "            if first_full == 1:\n",
    "                y_complete = y[:,:,:,0].unsqueeze(-1)\n",
    "                mask_complete = mask\n",
    "                y_pred_complete = y_pred\n",
    "\n",
    "            else:\n",
    "                y_complete = torch.cat((y_complete, y[:,:,:,0].unsqueeze(-1)))\n",
    "                mask_complete = torch.cat((mask_complete, mask))\n",
    "                y_pred_complete = torch.cat((y_pred_complete, y_pred))\n",
    "\n",
    "            first_full+=1\n",
    "\n",
    "        raw = (y_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "        mask = (mask_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "        pred = (y_pred_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "        \n",
    "        raw = raw[0:len(pvts.time_index)]\n",
    "        mask = mask[0:len(pvts.time_index)]\n",
    "        pred = pred[0:len(pvts.time_index)]\n",
    "\n",
    "else:\n",
    "    \n",
    "    first_full = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for x, y, mask in tqdm(full_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            # get model predictions and compute loss\n",
    "            y_pred = eval_model(x.to(device), \n",
    "                                    full_edge_index.to(device), \n",
    "                                    full_edge_weight.to(device)\n",
    "                                    )\n",
    "            \n",
    "            if first_full == 1:\n",
    "                y_complete = y[:,:,:,0].unsqueeze(-1)\n",
    "                mask_complete = mask\n",
    "                y_pred_complete = y_pred\n",
    "\n",
    "            else:\n",
    "                y_complete = torch.cat((y_complete, y[:,:,:,0].unsqueeze(-1)))\n",
    "                mask_complete = torch.cat((mask_complete, mask))\n",
    "                y_pred_complete = torch.cat((y_pred_complete, y_pred))\n",
    "\n",
    "            first_full+=1\n",
    "\n",
    "    raw = (y_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "    raw = np.where(raw == 0, float(.1), raw)\n",
    "\n",
    "    mask = (mask_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "    pred = (y_pred_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach().numpy()\n",
    "    \n",
    "    raw = raw[0:len(pvts.time_index)]\n",
    "    mask = mask[0:len(pvts.time_index)]\n",
    "    pred = pred[0:len(pvts.time_index)]\n",
    "\n",
    "    raw_val = raw\n",
    "    mask_val = mask\n",
    "    pred_val = pred\n",
    "# %% \n",
    "#Calculate Model Error\n",
    "raw = np.where(raw == 0, float(.1), raw)\n",
    "raw_val = np.where(raw_val == 0, float(.1), raw_val)\n",
    "\n",
    "MAE_val = np.nanmean((np.where(mask_val == False, np.abs(pred_val - raw_val), np.nan)))\n",
    "MAPE_val = np.nanmean((np.where(mask_val == False, np.abs((pred_val - raw_val) / raw_val), np.nan)))\n",
    "MSE_val = np.nanmean((np.where(mask_val == False, (pred_val - raw_val)**2, np.nan)))\n",
    "RMSE_val = ((np.nanmean((np.where(mask_val == False, (pred_val - raw_val)**2, np.nan))))**.5)\n",
    "\n",
    "MAE = np.nanmean((np.where(mask == False, np.abs(pred - raw), np.nan)))\n",
    "MAPE = np.nanmean((np.where(mask == False, np.abs((pred - raw) / raw), np.nan)))\n",
    "MSE = np.nanmean((np.where(mask == False, (pred - raw)**2,  np.nan)))\n",
    "RMSE = ((np.nanmean((np.where(mask == False, (pred - raw)**2,  np.nan))))**.5)\n",
    "\n",
    "# raw.to_parquet()\n",
    "# raw_val.to_parquet()\n",
    "# mask.to_parquet()\n",
    "# mask_val.to_parquet()\n",
    "# pred_val.to_parquet()\n",
    "# pred.to_parquet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model parameters and model results into a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df = pd.DataFrame({\n",
    "    'model_num' : model_num,\n",
    "    'MAE_val': MAE_val,\n",
    "    'MAPE_val': MAPE_val,\n",
    "    'MSE_val': MSE_val,\n",
    "    'RMSE_val': RMSE_val,\n",
    "    'MAE': MAE,\n",
    "    'MAPE': MAPE,\n",
    "    'MSE': MSE,\n",
    "    'RMSE': RMSE,\n",
    "    'train_columns': [pvts.train_cols],\n",
    "    'test_columns': [pvts.test_cols],\n",
    "    'val_columns' : [pvts.val_cols],\n",
    "    'train_indices': [pvts.train_windows],\n",
    "    'test_indices': [pvts.test_windows],\n",
    "    'val_indices' : [pvts.val_windows],\n",
    "    'train_loss_curve': [train_loss],\n",
    "    'test_loss_curve': [test_loss],\n",
    "    'model_save_path' : model_save_path\n",
    "})\n",
    "\n",
    "#merged_df = pd.concat([model_results_df, input_df.reset_index()], axis=1)\n",
    "model_results_df.to_csv(model_results_path)\n",
    "\n",
    "pd.DataFrame(pred_val).to_parquet(pred_test_save_path)\n",
    "pd.DataFrame(pred).to_parquet(pred_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_names = pvts.channel_dataframes[pvts.channel_indices['p_mp']].columns.values\n",
    "valid_set = pd.DataFrame(pred_val)\n",
    "valid_set.columns = sys_names[pvts.val_cols]\n",
    "valid_set.columns = valid_set.columns.str.replace('_p_mp', '')\n",
    "valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_set = valid_set.iloc[:-101]\n",
    "real_data = pvts.channel_dataframes[pvts.channel_indices['cdcp_fill']]\n",
    "real_data.columns = real_data.columns.str.replace('_PVDCPOWER', '')\n",
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = real_data[valid_set.columns]\n",
    "\n",
    "norms = np.nanmax(real_data, axis=0)\n",
    "valid_set = (valid_set / 100) * norms \n",
    "\n",
    "emp_data = pvts.channel_dataframes[pvts.channel_indices['p_mp']]\n",
    "emp_data.columns = emp_data.columns.str.replace('_p_mp', '')\n",
    "emp_data = emp_data[valid_set.columns]\n",
    "mask = pvts.channel_dataframes[pvts.channel_indices['mask']][pvts.val_cols].astype(bool)\n",
    "mask.columns = valid_set.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Random Selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for start in range(1, 200):\n",
    "for i, column_name in enumerate(real_data):\n",
    "    for start in range(0,len(real_data), 25000):\n",
    "        end = start + 300\n",
    "        subset_real = real_data.values[start:end,i]\n",
    "        subset_mask = mask.values[start:end,i]\n",
    "        subset_emp = emp_data.values[start:end,i]\n",
    "        subset_val = valid_set.values[start:end,i]\n",
    "        plt.plot(np.where(subset_mask == False,subset_real,0), 'o', label='Real Data')\n",
    "        plt.plot(subset_emp, label='Empirical Data')\n",
    "        plt.plot(subset_val, label='Validation Set')\n",
    "\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(f'Comparison of Subsets {column_name}')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
