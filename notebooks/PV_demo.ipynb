{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photovoltaic Fleet ddDT Demo\n",
    "This notebook is an example of training a ddDT on a fleet of PV systems.\n",
    "\n",
    "Before running this notebook, make sure the package is installed in your system by running \n",
    "`pip install -e .` from the base directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import os \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch_geometric.nn import ChebConv\n",
    "from tqdm import tqdm\n",
    "#from rwb_stgnn_functions import *\n",
    "#from rwb_dataloader_sunsmart import *\n",
    "from graphfoundationmodels.models.stGAE import STConvAE\n",
    "from graphfoundationmodels.dataloaders.dataloader_sunsmart import stDataloader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the complexity of this system, an integrated dataloader has been developed that operates off of config files. We'll load that in first.\n",
    "\n",
    "We also define a set folder structure for the data, with optional functionality for data preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = '../exemplar_PV/data/x.parquet'\n",
    "for column in input_df.columns:\n",
    "    globals()[column] = input_df[column].iloc[0]\n",
    "\n",
    "# Directorys\n",
    "meta_directory = '/mnt/vstor/CSE_MSE_RXF131/staging/sdle/foundational_models/sunsmart/sunsmart_meta_grade.parquet'\n",
    "\n",
    "#Dataloader\n",
    "pvts = stDataloader(parent_directory = '/mnt/vstor/CSE_MSE_RXF131/staging/sdle/foundational_models/sunsmart/',\n",
    "                            raw_parquet_path = '',\n",
    "                            node_parquet_path ='',\n",
    "                            channel_parquet_path = '',\n",
    "                            adjacency_parquet_path = '', \n",
    "                            meta_parquet_path = meta_directory,\n",
    "                            num_nodes = 90, \n",
    "                            traintestsplit= traintestsplit,\n",
    "                            splittype = splittype,\n",
    "                            normalize = True,\n",
    "                            model_output_directory='',\n",
    "                            test_name = test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define where to save results and model objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_' + str(start) + '.pt'\n",
    "model_save_path = pvts.model_path + model_name\n",
    "\n",
    "node_name = 'model_' + str(start) + '.csv'\n",
    "training_node_save_path = pvts.training_nodes_path + node_name\n",
    "\n",
    "window_name = 'model_' + str(start) + '.csv'\n",
    "training_window_save_path = pvts.training_windows_path + node_name\n",
    "\n",
    "train_name = 'model_' + str(start)  + '.csv' \n",
    "train_loss_save_path = pvts.train_loss_path + train_name\n",
    "\n",
    "test_name = 'model_' + str(start)  +'.csv'\n",
    "test_loss_save_path = pvts.test_loss_path + test_name\n",
    "\n",
    "error_test_name = 'model_' + str(start) + '.csv'\n",
    "error_test_save_path = pvts.error_path_test + error_test_name\n",
    "\n",
    "error_name = 'model_' + str(start) + '.csv'\n",
    "error_save_path = pvts.error_path + error_name\n",
    "\n",
    "pred_test_name = 'model_' + str(start) + '.parquet'\n",
    "pred_test_save_path = pvts.pred_test_path + pred_test_name\n",
    "\n",
    "pred_name = 'model_' + str(start) + '.parquet'\n",
    "pred_save_path = pvts.pred_path + pred_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's configure the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Timespan\n",
    "pvts.set_date_range('2012-09-28 07:00:00', '2015-10-08 05:30:00', '15min', tz = 'UTC')\n",
    "# %% Set Model Inputs\n",
    "pvts.set_window_size(n_his)\n",
    "pvts.set_model_inputs(inputs, outputs, masks, adjacency)\n",
    "pvts.set_train_test_splits(splits)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Transform the Datafrom tables to matrices\n",
    "train_data, test_data, eval_data, full_data = pvts.data_to_model_input_transform()\n",
    "train_iter = DataLoader(train_data, batch_size=int(batch_size),\n",
    "                    shuffle=shuffle, num_workers=2)\n",
    "test_iter = DataLoader(test_data, batch_size=int(batch_size),\n",
    "                    shuffle=False, num_workers=2)\n",
    "full_iter = DataLoader(full_data, batch_size=int(batch_size),\n",
    "                    shuffle=False, num_workers=2)\n",
    "\n",
    "# %% Transform the adjacency matrix tables to matrices\n",
    "train_w, train_edge_index, train_edge_weight, full_w, full_edge_index, full_edge_weight = pvts.weight_to_model_input_transform(datasplit = 'training')\n",
    "test_w, test_edge_index, test_edge_weight, _, _, _ = pvts.weight_to_model_input_transform(datasplit = 'testing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STConvAE(device, num_nodes, channels, num_layers, \n",
    "                    kernel_size, K, n_his, kernel_size_de, stride, \n",
    "                    padding, normalization = 'sym', bias = True)\n",
    "model = model.to(model.device)\n",
    "loss = nn.MSELoss()\n",
    "min_test_loss = np.inf\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Loop\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in tqdm(range(1, num_epochs + 1), desc = 'Epoch', position = 0):\n",
    "    epoch_train_loss = 0.0\n",
    "    model.train()\n",
    "    i = 0\n",
    "    \n",
    "    for x, y, mask in tqdm(train_iter, desc = 'Batch', position = 0):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y = y.to(model.device)\n",
    "        mask = mask.to(model.device)\n",
    "        y_pred = model(x.to(model.device), \n",
    "                        train_edge_index.to(model.device), \n",
    "                        train_edge_weight.to(model.device)\n",
    "                        ) \n",
    "        loss= torch.nanmean((torch.where(mask == False, \n",
    "                                (y_pred-y)**2, \n",
    "                                torch.tensor(float('nan'))\n",
    "                                ))\n",
    "                                )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() \n",
    "\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    epoch_test_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask in tqdm(test_iter, desc = 'Batch', position = 0):\n",
    "            y = y.to(model.device)\n",
    "            mask = mask.to(model.device)\n",
    "            y_pred = model(x.to(model.device),\n",
    "                                test_edge_index.to(model.device),\n",
    "                                test_edge_weight.to(model.device)\n",
    "                                ) \n",
    "            loss = torch.nanmean((torch.where(mask == False,\n",
    "                                    (y_pred-y)**2,\n",
    "                                    torch.tensor(float('nan'))\n",
    "                                    ))\n",
    "                                    )\n",
    "            epoch_test_loss += loss.item()\n",
    "        test_loss.append(epoch_test_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\n Training Loss: {epoch_train_loss / len(train_iter)} \\n Evaulation Loss: {epoch_test_loss / len(test_iter)}')\n",
    "    if min_test_loss > epoch_test_loss:\n",
    "        min_test_loss = epoch_test_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "pd.DataFrame(pvts.train_cols).to_csv(training_node_save_path)\n",
    "pd.DataFrame(pvts.train_windows).to_csv(training_window_save_path)\n",
    "pd.DataFrame(train_loss).to_csv(train_loss_save_path)\n",
    "pd.DataFrame(test_loss).to_csv(test_loss_save_path)\n",
    "# Model Evaluation\n",
    "\n",
    "#Intialize Model\n",
    "eval_model = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True).to(device)\n",
    "eval_model.load_state_dict(torch.load(model_save_path))\n",
    "eval_model = eval_model.to(eval_model.device)\n",
    "eval_loss = nn.MSELoss()\n",
    "first_eval = 1\n",
    "eval_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, mask in tqdm(test_iter, desc = 'Batch', position = 0):\n",
    "        torch.cuda.empty_cache()\n",
    "        # get model predictions and compute loss\n",
    "        y_pred = eval_model(x.to(device), test_edge_index.to(device), test_edge_weight.to(device))\n",
    "        if first_eval == 1:\n",
    "            y_complete_test = y\n",
    "            mask_complete_test = mask\n",
    "            y_pred_complete_test = y_pred\n",
    "        else:\n",
    "            y_complete_test = torch.cat((y_complete_test, y))\n",
    "            mask_complete_test = torch.cat((mask_complete_test, mask))\n",
    "            y_pred_complete_test = torch.cat((y_pred_complete_test, y_pred))\n",
    "        first_eval+=1\n",
    "\n",
    "first_full = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, mask in tqdm(full_iter, desc = 'Batch', position = 0):\n",
    "        torch.cuda.empty_cache()\n",
    "        # get model predictions and compute loss\n",
    "        y_pred = eval_model(x.to(device), full_edge_index.to(device), full_edge_weight.to(device))\n",
    "        if first_full == 1:\n",
    "            y_complete = y\n",
    "            mask_complete = mask\n",
    "            y_pred_complete = y_pred\n",
    "        else:\n",
    "            y_complete = torch.cat((y_complete, y))\n",
    "            mask_complete = torch.cat((mask_complete, mask))\n",
    "            y_pred_complete = torch.cat((y_pred_complete, y_pred))\n",
    "        first_full+=1\n",
    "\n",
    "raw_test = (y_complete_test).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach()\n",
    "mask_test = (mask_complete_test).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach()\n",
    "pred_test = (y_pred_complete_test).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach()\n",
    "raw_test = torch.where(raw_test == 0, torch.tensor(float(.1)), raw_test)\n",
    "\n",
    "raw = (y_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach()\n",
    "mask = (mask_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach()\n",
    "pred = (y_pred_complete).cpu().flatten(start_dim = 0, end_dim = 1).flatten(start_dim = 1, end_dim = 2).detach()\n",
    "raw = torch.where(raw == 0, torch.tensor(float(.1)), raw)\n",
    "\n",
    "#Calculate Model Error\n",
    "MAE_test = torch.nanmean((torch.where(mask_test == False, torch.abs(pred_test - raw_test), torch.tensor(float('nan'))))).detach().numpy()\n",
    "MAPE_test = torch.nanmean((torch.where(mask_test == False, torch.abs((pred_test - raw_test) / raw_test), torch.tensor(float('nan'))))).detach().numpy()\n",
    "MSE_test = torch.nanmean((torch.where(mask_test == False, (pred_test - raw_test)**2, torch.tensor(float('nan'))))).detach().numpy()\n",
    "RMSE_test = ((torch.nanmean((torch.where(mask_test == False, (pred_test - raw_test)**2, torch.tensor(float('nan'))))))**.5).detach().numpy()\n",
    "errors_test = [MAE_test,MAPE_test,MSE_test,RMSE_test]\n",
    "\n",
    "MAE = torch.nanmean((torch.where(mask == False, torch.abs(pred - raw), torch.tensor(float('nan'))))).detach().numpy()\n",
    "MAPE = torch.nanmean((torch.where(mask == False, torch.abs((pred - raw) / raw), torch.tensor(float('nan'))))).detach().numpy()\n",
    "MSE = torch.nanmean((torch.where(mask == False, (pred - raw)**2, torch.tensor(float('nan'))))).detach().numpy()\n",
    "RMSE = ((torch.nanmean((torch.where(mask == False, (pred - raw)**2, torch.tensor(float('nan'))))))**.5).detach().numpy()\n",
    "errors = [MAE,MAPE,MSE,RMSE]\n",
    "\n",
    "pred_test = pred_test.numpy()\n",
    "pred = pred.numpy()\n",
    "\n",
    "pd.DataFrame(errors).to_csv(error_save_path)\n",
    "pd.DataFrame(errors_test).to_csv(error_test_save_path)\n",
    "pd.DataFrame(pred_test).to_parquet(pred_test_save_path)\n",
    "pd.DataFrame(pred).to_parquet(pred_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
